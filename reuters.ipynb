{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install beautifulsoup4\n",
    "# !pip install requests\n",
    "# !pip install pandas"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Establishing Reuters Home page as the url to scrape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.reuters.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(url)\n",
    "soup = BeautifulSoup(r.content, \"html.parser\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape Categories:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The getCategories() function uses the BeautifulSoup library to extract all the categories available on the Reuters website. It extracts the category name and its corresponding link, and stores them in a list called categoriesLinks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCategories(url):\n",
    "    categories = soup.find_all(\"a\",{\n",
    "        \"class\":\"text__text__1FZLe text__dark-grey__3Ml43 text__medium__1kbOh text__default__UPMUu nav-bar__link__3mja8\"\n",
    "    })\n",
    "    categoriesLinks =  []\n",
    "    for category in categories:\n",
    "        categoriesLinks.append({\n",
    "            \"category\":category.text.strip(),\n",
    "            \"link\":category['href']\n",
    "        })\n",
    "    return categoriesLinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = getCategories(url)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape Topics and Menus:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The getTopicsAndMenus() function scrapes news topics and their menus from each category. For each category, the function first appends the category link to the main Reuters website link. It then requests the category link, scrapes the HTML content of the page, and extracts all the news topics available. It then loops through each topic and appends it to a pandas DataFrame called df as a new row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTopicsAndMenus(url,categoriesLinks, df):\n",
    "    for category in categoriesLinks:\n",
    "        category[\"link\"] = url+category[\"link\"]\n",
    "        request = requests.get(category[\"link\"])\n",
    "        categorySoup= BeautifulSoup(request.content, \"html.parser\")\n",
    "        buttons = categorySoup.find_all(\n",
    "            \"button\",\n",
    "            {\"class\":\"button__button__2Ecqi button__secondary__18moI button__pill__2LA8V text-button__container__3q3zX\"}\n",
    "            )\n",
    "        category[\"topics\"] = []\n",
    "        for button in buttons: \n",
    "            topic = button.find(\n",
    "                \"span\",\n",
    "                {\"class\":\"text__text__1FZLe text__inherit-color__3208F text__bold__2-8Kc text__default__UPMUu text-button__medium__113uZ\"}\n",
    "                )\n",
    "            df = pd.concat([df,pd.DataFrame([{\"Category\": category[\"category\"],\"CategoryLink\": category[\"link\"], \"Menus\": button[\"data-id\"], \"Topics\": topic.text.strip()}])],ignore_index=True)\n",
    "    df[\"topicLink\"] = url+df[\"Menus\"]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=[\"Category\",\"CategoryLink\", \"Menus\", \"Topics\"])\n",
    "df = getTopicsAndMenus(url,categories, df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape Article Links, Titles, Categories, and Topics:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The getArtciles() function scrapes article links, titles, categories, and topics. It uses the topic link from the getTopicsAndMenus() function to extract article links, titles, and their respective topics. It then appends the extracted data to the df pandas DataFrame as new columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getArtciles(df,url):\n",
    "    data = []\n",
    "    for topicLink in df[\"topicLink\"]:\n",
    "        request = requests.get(topicLink)\n",
    "        topicSoup= BeautifulSoup(request.content, \"html.parser\")\n",
    "        topicCards = topicSoup.find_all(\n",
    "            \"div\",\n",
    "            {\"class\":\"media-story-card__hub__3mHOR story-card\"}\n",
    "        )\n",
    "        for card in topicCards:\n",
    "            categories = card.find(\"a\",{\n",
    "                \"class\" : \"text__text__1FZLe text__inherit-color__3208F text__inherit-font__1Y8w3 text__inherit-size__1DZJi link__underline_on_hover__2zGL4\"\n",
    "            })\n",
    "            linksOfTitles = card.find(\"a\",{\n",
    "                \"class\":\"text__text__1FZLe text__dark-grey__3Ml43 text__medium__1kbOh text__heading_5_and_half__3YluN heading__base__2T28j heading_5_half media-story-card__heading__eqhp9\"\n",
    "            })\n",
    "            time = card.find(\"time\",{\"class\":\"text__text__1FZLe text__inherit-color__3208F text__regular__2N1Xr text__extra_small__1Mw6v label__label__f9Hew label__small__274ei media-story-card__time__2i9EK\"})\n",
    "            data.append({\n",
    "                \"Title\":linksOfTitles.find(\"span\").text.strip(),\n",
    "                \"ArticleLink\":(url+linksOfTitles[\"href\"]) if linksOfTitles[\"href\"] is not None else None,\n",
    "                \"Topics\":next(categories.stripped_strings) if categories is not None else None\n",
    "            })\n",
    "    data = pd.DataFrame(data)\n",
    "    df = pd.merge(df, data, on=['Topics'], how='outer')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = getArtciles(df,url)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the Data:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tempDf DataFrame is used to extract website, category, and topic information from the article links. It then updates the df DataFrame by replacing null values with the extracted information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempDf = pd.DataFrame()\n",
    "tempDf[['website', 'category', 'topic']] = df['ArticleLink'].str.split('/', expand=True)[[2,3,4]]\n",
    "mask = df[['Category', 'CategoryLink', 'Menus', 'topicLink']].isna().all(axis=1)\n",
    "df.loc[mask, 'Menus'] = '/' + tempDf['category'] + '/'+ tempDf['topic'] +'/'\n",
    "df.loc[mask, 'Category'] = tempDf['category']\n",
    "df.loc[mask, 'CategoryLink'] = 'https://'+tempDf[\"website\"]+\"/\"+tempDf[\"category\"]+\"/\"\n",
    "df.loc[mask, 'topicLink'] = 'https://'+tempDf[\"website\"]+df[mask]['Menus']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset=['ArticleLink'], inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape Article Details:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The getArticleDetails() function scrapes the article details for each article link in the df DataFrame. It extracts the article image, article text, authors, and date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getArticleDetails(df):\n",
    "    data = []\n",
    "    for articleLink in df[df['ArticleLink'].notnull()]['ArticleLink']:\n",
    "        if articleLink is not None:\n",
    "            request = requests.get(articleLink)\n",
    "            articleSoup= BeautifulSoup(request.content, \"html.parser\")\n",
    "            image = articleSoup.find(\"div\",{\n",
    "                \"class\":\"styles__image-container__skIG1 styles__fill__3xCr1 styles__center_center__1AaPV styles__apply-ratio__1_FYQ\"\n",
    "            })\n",
    "            timeContainer = articleSoup.find(\"time\",{\n",
    "                \"class\":\"text__text__1FZLe text__dark-grey__3Ml43 text__regular__2N1Xr text__extra_small__1Mw6v article-header__dateline__4jE04\"\n",
    "            })\n",
    "            title = articleSoup.find(\"div\",{\n",
    "                \"class\":\"article-header__heading__15OpQ\"\n",
    "            })\n",
    "            author = articleSoup.find(\"a\",{\n",
    "                \"class\":\"author-name__author__1gx5k\"\n",
    "            })\n",
    "            articleBodyContainer = articleSoup.find(\"div\",{\n",
    "                \"class\":\"article-body__content__17Yit paywall-article\"\n",
    "            })\n",
    "            \n",
    "            artcileBody  = articleBodyContainer.find_all(\"p\") if articleBodyContainer is not None else None\n",
    "            wholeBody = \"\"\n",
    "            if artcileBody is not None:\n",
    "                for body in artcileBody:\n",
    "                    wholeBody+=body.text+\"\\n\"\n",
    "            if timeContainer is not None:\n",
    "                date = timeContainer.find_all(\"span\")[1].text\n",
    "                time = timeContainer.find_all(\"span\")[2].text\n",
    "            else:\n",
    "                date = None\n",
    "                time = None\n",
    "            image = image.find(\"img\")[\"src\"] if image is not None else None\n",
    "            data.append({\n",
    "                \"ArticleLink\":articleLink,\n",
    "                \"Title\":title.find(\"h1\").text if title is not None else None,\n",
    "                \"Author\":author.text if author is not None else None,\n",
    "                \"Date\":date,\n",
    "                \"Time\":time,\n",
    "                \"Image\":image,\n",
    "                \"Article\":wholeBody\n",
    "                })\n",
    "    data = pd.DataFrame(data)\n",
    "    df = pd.merge(df, data, on=['ArticleLink'], how='outer')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = getArticleDetails(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempDf = pd.DataFrame()\n",
    "tempDf['topic'] = df['topicLink'].str.split('/', expand=True)[4]\n",
    "mask = df[['Topics']].isna().all(axis=1)\n",
    "df.loc[mask, 'Topics'] = tempDf['topic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./reutersData.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_concat = pd.concat([df, data]).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./reutersData.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
